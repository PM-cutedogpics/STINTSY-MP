{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a19fc916",
   "metadata": {},
   "source": [
    "# Description of the dataset and the task\n",
    "- Data Collection\n",
    "- Implications on the types of conclusions that could be made from the data\n",
    "- Description of the variables, observations, and/or structure of the data\n",
    "- Target task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe5f78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os.path\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "import PIL.Image\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import dask.bag as bag\n",
    "from dask.diagnostics import ProgressBar\n",
    "import matplotlib.cm as cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13168578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list with the filepaths for training and testing\n",
    "batch_size = 32\n",
    "img_height = 224\n",
    "img_width = 224\n",
    "\n",
    "train_path = Path('dataset/train')\n",
    "test_path = Path('dataset/test')\n",
    "\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    train_path, \n",
    "    validation_split=0.2, \n",
    "    subset=\"training\", \n",
    "    seed=123, \n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size)\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  train_path,\n",
    "  validation_split=0.2,\n",
    "  subset=\"validation\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)\n",
    "\n",
    "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    test_path, \n",
    "    seed=123, \n",
    "    image_size=(img_height, img_width))\n",
    "\n",
    "class_names = train_ds.class_names\n",
    "print(class_names)\n",
    "print(len(class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d566179",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = np.array(train_ds.class_names)\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d893e070",
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_batch, labels_batch in train_ds:\n",
    "    print(image_batch.shape)\n",
    "    print(labels_batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205c84fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in train_ds.take(1):\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "        plt.title(class_names[labels[i]])\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f382a8",
   "metadata": {},
   "source": [
    "# Exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcdbb1e",
   "metadata": {},
   "source": [
    "This block of code initializes file paths for the folders and images that are going to be used for the exploratory data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52c7d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dir = Path('dataset/train')\n",
    "# train_filepaths_jpg = list(train_dir.rglob(r'**/*.jpg'))\n",
    "# train_filepaths_jpeg = list(train_dir.rglob(r'**/*.jpeg'))\n",
    "# train_filepaths_png = list(train_dir.rglob(r'**/*.png'))\n",
    "# train_filepaths = train_filepaths_jpg + train_filepaths_jpeg + train_filepaths_png\n",
    "\n",
    "\n",
    "# def proc_img(filepath):\n",
    "#     \"\"\" Create a DataFrame with the filepath and the labels of the pictures\n",
    "#     \"\"\"\n",
    "\n",
    "#     labels = [str(filepath[i]).split(\"\\\\\")[-2] \\\n",
    "#               for i in range(len(filepath))]\n",
    "\n",
    "#     filepath = pd.Series(filepath, name='Filepath').astype(str)\n",
    "#     labels = pd.Series(labels, name='Label')\n",
    "\n",
    "#     # Concatenate filepaths and labels\n",
    "#     df = pd.concat([filepath, labels], axis=1)\n",
    "\n",
    "#     # Shuffle the DataFrame and reset index\n",
    "#     df = df.sample(frac=1).reset_index(drop = True)\n",
    "    \n",
    "#     return df\n",
    "\n",
    "# train_df = proc_img(train_filepaths)\n",
    "# train_df = train_df.sort_values(\"Label\")\n",
    "# labels = train_df[\"Label\"].unique()\n",
    "# labels.sort()\n",
    "# # train_df\n",
    "# # train_df.loc[train_df[\"Label\"] == label]\n",
    "\n",
    "# i=0\n",
    "# imagePath = {}\n",
    "\n",
    "# for label in labels:\n",
    "#     imagePath[i] = [fn for fn in os.listdir(f'{train_dir}\\\\{label}') if fn.endswith('.jpg')]\n",
    "#     temp = [fn for fn in os.listdir(f'{train_dir}\\\\{label}') if fn.endswith('.png')]\n",
    "#     for j in temp:\n",
    "#         imagePath[i].append(j)\n",
    "#     temp = [fn for fn in os.listdir(f'{train_dir}\\\\{label}') if fn.endswith('.jpeg')]\n",
    "#     for j in temp:\n",
    "#         imagePath[i].append(j)\n",
    "#     i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5464df0",
   "metadata": {},
   "source": [
    "# Distribution of Sizes\n",
    "The distribution of sizes is shown by obtaining the dimensions of the image namely its height, weight, and depth if it has depth. The height and weight are then compiled into a scatterplot for easy visualization with each fruit and vegetable having its own graph. Knowing the distribution of sizes can help us understand which size is appropriate to be used for pre-processing purposes to normalize the sizes of each image and better tune the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce1412f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Distribution of Sizes\n",
    "# classDirectories = {}\n",
    "# for label in labels:\n",
    "#     classDirectories[label] = 'dataset/train/' + label + '/'\n",
    "\n",
    "# def get_dims(file):\n",
    "#     im = Image.open(file)\n",
    "#     arr = np.array(im)\n",
    "#     if (len(arr.shape) == 3):\n",
    "#         h,w,d = arr.shape\n",
    "#     else:\n",
    "#         h,w = arr.shape\n",
    "#     return h,w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c01a4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for n,d in classDirectories.items():\n",
    "#     filepath = d\n",
    "#     filelist = [filepath + f for f in os.listdir(filepath)]\n",
    "#     dims = bag.from_sequence(filelist).map(get_dims)\n",
    "#     with ProgressBar():\n",
    "#         dims = dims.compute()\n",
    "#         dim_df = pd.DataFrame(dims, columns=['height', 'width'])\n",
    "#         sizes = dim_df.groupby(['height', 'width']).size().reset_index().rename(columns={0:'count'})\n",
    "#         sizes.plot.scatter(x='width', y='height');\n",
    "#         plt.xlim(0, 8000)\n",
    "#         plt.ylim(0, 8000)\n",
    "#         plt.title('Image Sizes (pixels) | {}'.format(n))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd2149c",
   "metadata": {},
   "source": [
    "It can be seen from the results above that most images gather around the 0-2000 height and width so it is important to run it again and limit the view to those of that size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911c8a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for n,d in classDirectories.items():\n",
    "#     filepath = d\n",
    "#     filelist = [filepath + f for f in os.listdir(filepath)]\n",
    "#     dims = bag.from_sequence(filelist).map(get_dims)\n",
    "#     with ProgressBar():\n",
    "#         dims = dims.compute()\n",
    "#         dim_df = pd.DataFrame(dims, columns=['height', 'width'])\n",
    "#         sizes = dim_df.groupby(['height', 'width']).size().reset_index().rename(columns={0:'count'})\n",
    "#         sizes.plot.scatter(x='width', y='height');\n",
    "#         plt.xlim(0, 2000)\n",
    "#         plt.ylim(0, 2000)\n",
    "#         plt.title('Image Sizes (pixels) | {}'.format(n))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cedeb29",
   "metadata": {},
   "source": [
    "It can be observed that most images land in the size 250x250 to 750x750"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2d33ab",
   "metadata": {},
   "source": [
    "# Distribution of Labels\n",
    "By checking the directories of each label the amount of images per fruit and vegetable can be shown in a bar graph with number of images in the y-axis and label in the x-axis. Knowing the proper distrubtion of labels can help determine which image has a different count from others. It is important for each label to have the same amount of images as it may affect the training of the model later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00b5a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Distribution of Labels\n",
    "# number_classes = {}\n",
    "# for label in labels:\n",
    "#     number_classes[label] = len(os.listdir(f'{train_dir}\\\\{label}'))\n",
    "# f, ax = plt.subplots(figsize=(33,20)) # set the size that you'd like (width, height)\n",
    "# plt.bar(number_classes.keys(), number_classes.values(), width = .5)\n",
    "# plt.title(\"Number of Images by Class\");\n",
    "# plt.xlabel('Class Name');\n",
    "# plt.ylabel('# Images');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5c3179",
   "metadata": {},
   "source": [
    "# Data Pre-processing and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a8e18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import PIL.Image as Image\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "import datetime\n",
    "\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c538fb19",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1384a1f",
   "metadata": {},
   "source": [
    "We will make use of **MobileNetV2**, a pre-trained network for image classification. Through this network, **transfer learning** will be performed to make the classification of fruits and vegetables much easier.\n",
    "\n",
    "We begin by extracting the labels from **ImageNet**, a database of images, to initially test out the their labels on our dataset. We also extract MobileNetV2 for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1175d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "inception_v3 = \"https://tfhub.dev/google/tf2-preview/inception_v3/feature_vector/4\"\n",
    "\n",
    "classifier_model = inception_v3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e62e960",
   "metadata": {},
   "source": [
    "We define the size of the images and create a **sequential model**. This type of model means building the network one layer at a time. MobileNetV2 is wrapped in a Keras layer to be called later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadeb1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SHAPE = (224, 224)\n",
    "\n",
    "classifier = tf.keras.Sequential([\n",
    "    hub.KerasLayer(classifier_model, input_shape=IMAGE_SHAPE+(3,))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ee3008",
   "metadata": {},
   "source": [
    "We normalize the values of the input betwee [0, 1] and apply it into our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c950ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
    "train_ds = train_ds.map(lambda x, y: (normalization_layer(x), y)) # Where x—images, y—labels.\n",
    "val_ds = val_ds.map(lambda x, y: (normalization_layer(x), y)) # Where x—images, y—labels.\n",
    "test_ds = test_ds.map(lambda x, y: (normalization_layer(x), y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fba3a6b",
   "metadata": {},
   "source": [
    "While the input pipeline is running, optimization algorithms are implemented to monitor the CPU allocation and tune the value dynamically at runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e5bef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013729bc",
   "metadata": {},
   "source": [
    "Test datasets are then shuffled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71965c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_batch, labels_batch in train_ds:\n",
    "    print(image_batch.shape)\n",
    "    print(labels_batch.shape)\n",
    "    break\n",
    "\n",
    "for test_image_batch, test_labels_batch in test_ds:\n",
    "    print(image_batch.shape)\n",
    "    print(labels_batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae52e685",
   "metadata": {},
   "source": [
    "As observed, making use of labels from ImageNet was not a good idea as some labels exists in ImageNet but not on our dataset. \n",
    "\n",
    "Now, we will make use of MobileNetV2 against our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3ce5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "inception_v3 = \"https://tfhub.dev/google/tf2-preview/inception_v3/feature_vector/4\"\n",
    "\n",
    "feature_extractor_model = inception_v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58769507",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor_layer = hub.KerasLayer(\n",
    "    feature_extractor_model,\n",
    "    input_shape=(224, 224, 3),\n",
    "    trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afe173b",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_batch = feature_extractor_layer(image_batch)\n",
    "print(feature_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2237d5d7",
   "metadata": {},
   "source": [
    "Again, we create a model from the pre-trained model with an additiona of a dense layer. A dense layer is a hidden layer that receives input from all neurons of its previous layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91c9802",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(class_names)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "  feature_extractor_layer,\n",
    "  tf.keras.layers.Dense(num_classes)\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20dc8217",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model(image_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eaea490",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6fcaa8",
   "metadata": {},
   "source": [
    "The model will be configured with **Adam** as it's optimizer as it is the best adapative optimizer for sparse data. It makes use of stochastic gradient descent method that is based on adaptive estimation of first-order and second-order moments. We make use of **SparseCategoricalCrossentropy** as its loss function since we have more than 1 classes.\n",
    "\n",
    "A visualization of the events will also be shown which includes: Metrics summary plots. Training graph visualization. Weight histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d67ff0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(),\n",
    "  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "  metrics=['acc'])\n",
    "\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir=log_dir,\n",
    "    histogram_freq=1) # Enable histogram computation for every epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357572c1",
   "metadata": {},
   "source": [
    "The number of epochs was initially set to 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcccd9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 20\n",
    "\n",
    "history = model.fit(train_ds,\n",
    "                    validation_data=val_ds,\n",
    "                    epochs=NUM_EPOCHS,\n",
    "                    callbacks=tensorboard_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa9e427",
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b8b512",
   "metadata": {},
   "source": [
    "Upon observing the change in accuracy of the model, it can be observed that upon reaching 9 epochs, the accuracy no longer improves and the graph goes along the horizontal axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffa7c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(test_ds)\n",
    "print('Test accuracy :', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1f2cd9",
   "metadata": {},
   "source": [
    "Testing the model against the test dataset, a **loss  of 0.5047 and an accuracy of 0.8576** was achieved.\n",
    "\n",
    "We now test the model the against a batch of the train dataset and plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ebd0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_batch = model.predict(image_batch)\n",
    "predicted_id = tf.math.argmax(predicted_batch, axis=-1)\n",
    "predicted_label_batch = class_names[predicted_id]\n",
    "print(predicted_label_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad274ac0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,20))\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "\n",
    "for n in range(30):\n",
    "    plt.subplot(6,5,n+1)\n",
    "    plt.imshow(image_batch[n])\n",
    "    plt.title(predicted_label_batch[n].title())\n",
    "    plt.axis('off')\n",
    "_ = plt.suptitle(\"Model predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1946d0",
   "metadata": {},
   "source": [
    "Again, it will be tested against a batch of the test dataset and will be plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db72f385",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = test_ds.shuffle(2)\n",
    "for test_image_batch, test_labels_batch in test_ds:\n",
    "    print(image_batch.shape)\n",
    "    print(labels_batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f46db69",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_batch = model.predict(test_image_batch)\n",
    "predicted_id = tf.math.argmax(predicted_batch, axis=-1)\n",
    "predicted_label_batch = class_names[predicted_id]\n",
    "print(predicted_label_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7214edc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,20))\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "\n",
    "for n in range(30):\n",
    "    plt.subplot(6,5,n+1)\n",
    "    plt.imshow(test_image_batch[n])\n",
    "    plt.title(predicted_label_batch[n].title())\n",
    "    plt.axis('off')\n",
    "_ = plt.suptitle(\"Model predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8dbe18a",
   "metadata": {},
   "source": [
    "To avoid time being consumed while training the data, the model will be saved for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d27b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time.time()\n",
    "\n",
    "export_path = \"saved_models/{}\".format(int(t))\n",
    "model.save(export_path)\n",
    "\n",
    "export_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78db6322",
   "metadata": {},
   "outputs": [],
   "source": [
    "reloaded = tf.keras.models.load_model(export_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc9ebb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_batch = model.predict(image_batch)\n",
    "reloaded_result_batch = reloaded.predict(image_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e27c6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "abs(reloaded_result_batch - result_batch).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552ee6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "reloaded_predicted_id = tf.math.argmax(reloaded_result_batch, axis=-1)\n",
    "reloaded_predicted_label_batch = class_names[reloaded_predicted_id]\n",
    "print(reloaded_predicted_label_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85498da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,20))\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "for n in range(30):\n",
    "    plt.subplot(6,5,n+1)\n",
    "    plt.imshow(image_batch[n])\n",
    "    plt.title(reloaded_predicted_label_batch[n].title())\n",
    "    plt.axis('off')\n",
    "_ = plt.suptitle(\"Model predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ec45ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on the test dataset\n",
    "reloaded_result_batch = reloaded.predict(test_image_batch)\n",
    "reloaded_predicted_id = tf.math.argmax(reloaded_result_batch, axis=-1)\n",
    "reloaded_predicted_label_batch = class_names[reloaded_predicted_id]\n",
    "print(reloaded_predicted_label_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6d43b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,20))\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "for n in range(30):\n",
    "    plt.subplot(6,5,n+1)\n",
    "    plt.imshow(test_image_batch[n])\n",
    "    plt.title(reloaded_predicted_label_batch[n].title())\n",
    "    plt.axis('off')\n",
    "_ = plt.suptitle(\"Model predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6392bee",
   "metadata": {},
   "source": [
    "# Model Selection and Hyperparameter Tuning\n",
    "- Change number of epochs\n",
    "- Change the batch size\n",
    "- Add/reduce the number of layers\n",
    "- Change activation function in dense layer\n",
    "- Change kernal/bias(?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98efd599",
   "metadata": {},
   "source": [
    "# Insights and conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8e8062",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
